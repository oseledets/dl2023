{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UBCQSvnL9J0"
   },
   "source": [
    "# Problem Set 1 (20 + 40 + 30 = 90 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThKPiYZdMIqv"
   },
   "source": [
    "## Important information\n",
    "1.  We provide signatures of the functions that you have to implement. Make sure you follow the signatures defined, otherwise your coding solutions will not be graded.\n",
    "\n",
    "2.  Please submit the single Jupyter Notebook file, where only Python and Markdown/$\\LaTeX$ are used. Any hand-written solutions inserted by photos or in any other way **are prohibitive and will not be graded**. If you will have any questions about using Markdown, ask them!\n",
    "3.  The works will be checked for **plagiarism**. The score will be divided by the number of similar works.\n",
    "\n",
    "4. Some tasks in this assignment are bonus. It means that you can get maximum score for this assignment even if you ignore these tasks. However, if you get $y$ points for bonus tasks and $x$ points for regular tasks, then your score for the assignment will be $\\min(90, x+y)$. If $x + y > 90$, then we will label your submission and take into account in the final grade releasing (e.g. in the border case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdiykEAZUB8b"
   },
   "source": [
    "## Problem 1. QR backpropagation (20 pts)\n",
    "\n",
    "Our goal is to write an orthogonalization layer, that for an arbitrary rectangular matrix $A$ produces an orthonormal matrix $Q$ and (optionally) an upper triangualr matrix $R$ such that $A = QR$. \n",
    "In other words, we need to compute the QR decomposition.\n",
    "\n",
    "There are problems when one need to optimize over orthogonal matrices, so using such layer can be a solution.\n",
    "\n",
    "A standard algorithm to compute the QR decomposition is the Gram-Schmidt process.\n",
    "For the linearly independent set of vectors $a_1,\\dots,a_n$ (that are defined with the matrix $A$) it computes matrix $Q$ that contains orthonormalized set of vectors $q_1,\\dots,q_n$.\n",
    "\n",
    "This algorithm is listed below.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "u_1 &= v_1, \\\\\n",
    "u_2 &= v_2 - \\frac{(v_2, u_1)}{(u_1, u_1)} u_1, \\\\\n",
    "\\dots \\\\\n",
    "u_n &= v_n - \\frac{(v_n, u_1)}{(u_1, u_1)} u_1 - \\frac{(v_n, u_2)}{(u_2, u_2)} u_2 - \\dots - \\frac{(v_n, u_{n-1})}{(u_{n-1}, u_{n-1})} u_{n-1}.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Obtained $u_1, \\dots, u_n$ are orthogonal vectors in exact arithmetics. Then to make the system orthonormal you should divide each of the vectors by its norm: $u_i := u_i/\\|u_i\\|$.\n",
    "\n",
    "We have $A = QR$. \n",
    "Matrix $Q$ consists of vectors $u_1, \\dots, u_n$. \n",
    "Now let's look at the matrix $R$. \n",
    "\n",
    "$Q^*A = R$. \n",
    "So $R_{i, j} = (u_i, v_j)$ (assuming $u_i$ is already normalized). \n",
    "And it is exactly what we see in Gram-Schmidt algorithm:\n",
    "\n",
    "$$ v_n = \\frac{(v_n, u_1)}{(u_1, u_1)} u_1 + \\frac{(v_n, u_2)}{(u_2, u_2)} u_2 + \\dots + \\frac{(v_n, u_{n-1})}{(u_{n-1}, u_{n-1})} u_{n-1} + u_n.$$ \n",
    "\n",
    "This is the representation of vectors $v_i$ in our newly computed ortonormal basis. \n",
    "Thus they form matrix $R$. \n",
    "These coefficients are in the upper triangular part of $R$, because when $i>j$ we get $R_{i j} = 0$, because then we get sum of scalar products of orthogonal vectors. \n",
    "\n",
    "There is a more [computationally stable version of the Gram-Schmidt algorithm](https://en.wikipedia.org/wiki/Gram–Schmidt_process#Numerical_stability) that reduces the loss of orthogonality in matrix $Q$. \n",
    "This algorithm is called the modified Gram-Schmidt algorithm.\n",
    "\n",
    "The loop of orthogonalization is the same, except for the computing the vector $u_k$ simultaneously:\n",
    "\n",
    "$$ u_n = v_n - \\frac{(v_n, u_1)}{(u_1, u_1)} u_1 - \\frac{(v_n, u_2)}{(u_2, u_2)} u_2 - \\dots - \\frac{(v_n, u_{n-1})}{(u_{n-1}, u_{n-1})} u_{n-1}\n",
    "u_n$$\n",
    "\n",
    "$$ \n",
    " = v_n - \\frac{(v_n, u_1)}{(u_1, u_1)} u_1 - \\frac{(v_n, u_2)}{(u_2, u_2)} u_2 - \\dots - \\frac{(v_n, u_{n-1})}{(u_{n-1}, u_{n-1})} u_{n-1},\n",
    "$$\n",
    "\n",
    "one need to iteratively update $u_k$ step-by-step for a better numerical stability.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "u_k &= v_k - \\frac{(v_k, u_1)}{(u_1, u_1)} u_1, \\\\\n",
    "u_k &= u_k - \\frac{(u_k, u_2)}{(u_2, u_2)} u_2, \\\\\n",
    "\\dots \\\\\n",
    "u_k &= u_k - \\frac{(u_k, u_{k-1})}{(u_{k-1}, u_{k-1})} u_{k-1}. \\\\\n",
    "\\end{split}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QcIXodrhzdE_"
   },
   "source": [
    "You are supposed to do the following tasks:\n",
    "\n",
    "__1.__ (2 points) Implement the modified Gram-Schmidt algorithm in a function ```modified_gram_schmidt(A)``` using PyTorch.\n",
    "\n",
    "__2.__ (1 point) Create a random matrix $A \\in \\mathbb{R}^{m \\times n}$, $m \\gg n$. For the arbitrary loss function, eg. $L(Q) = \\sum_{i=0}^{m-1}\\sum_{j=0}^{n-1} q_{ij}$, compute the gradient $\\frac{\\partial L}{\\partial Q}$. Finally, compute $\\frac{\\partial L}{\\partial A}$ using built-in PyTorch backpropagation through your implementation of QR decomposition (```modified_gram_schmidt(A)```).\n",
    "\n",
    "__3.__ (2 points) Compute gradient $\\frac{\\partial L}{\\partial A}$ using backpropagation through built-in PyTorch function for computing QR decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsQwnQOR0x4f"
   },
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install memory-profiler\n",
    "import torch\n",
    "\n",
    "def modified_gram_schmidt(A):\n",
    "    \"\"\"\n",
    "    Computes QR decomposition of matrix A\n",
    "    \n",
    "    Input: \n",
    "        A - n x m matrix\n",
    "    Output:\n",
    "        Q - n x m orthonormal matrix\n",
    "        R - m x m upper triangular matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    return Q, R\n",
    "\n",
    "m = 100\n",
    "n = 20\n",
    "A = torch.rand((m, n))\n",
    "\n",
    "# Your code is below\n",
    "Q1, R1 = # torch version\n",
    "Q2, R2 = # modified_gram_schmidt\n",
    "\n",
    "loss1 = Q1.sum()\n",
    "loss2 = Q2.sum()\n",
    "\n",
    "# TODO compute gradients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Inu5xtKg1MMv"
   },
   "source": [
    "__4.__ (13 points total) Write the backpropagation $\\frac{\\partial L}{\\partial A}$ from scratch with PyTorch. Do not forget to use ```requires_grad=False``` to disable PyTorch autograd accumulation of the gradient.\n",
    "\n",
    "  a) (2 points) Derive analytically the gradient $\\frac{\\partial y}{\\partial x}$ of vector normalization operation $y := x/\\|x\\|$.\n",
    "\n",
    "  b) (1 point) Implement backward through the vector normalization operation (can be implemented either in the function ```backward_normalization``` or further inside the code).\n",
    "\n",
    "  c) (2 points) Derive analytically the gradient $\\frac{\\partial u_k}{\\partial v_j}$ of the othonormalization operation:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "u_k &= v_k - \\frac{(v_k, u_1)}{(u_1, u_1)} u_1, \\\\\n",
    "u_k &= u_k - \\frac{(u_k, u_2)}{(u_2, u_2)} u_2, \\\\\n",
    "\\dots \\\\\n",
    "u_k &= u_k - \\frac{(u_k, u_{k-1})}{(u_{k-1}, u_{k-1})} u_{k-1}. \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "  d) (2 points) Implement backward through the othonormalization operation (can be implemented either in the function ```backward_orthogonalization``` or further inside the code).\n",
    "\n",
    "  e) (1 point) Implement the QR forward pass (almost identically to ```modified_gram_schmidt```, but with storing the additional data nesessary for backpropagation) with the method ```forward``` of class ```QR``` (see template below).\n",
    "\n",
    "  f) (5 points) Implement the QR backward pass using previously implemented functions and place it in the method ```backward``` of class ```QR```  (see template below).\n",
    "\n",
    "__5.__ (2 points) Look at obtained $Q$ and $R$ matrices for each approach (steps 2, 3, 4), verify that $A \\approx QR$ and $Q^TQ \\approx I$. Measure the required memory for backpropagation (eg. with ```memory_profiler```). \n",
    "Compare the results of three approaches: \n",
    "\n",
    "- gradient from the PyTorch built-in function\n",
    "- gradient from PyTorch autograd applied to your function ```modified_gram_schmidt```\n",
    "- your custom implementation of backward step from analytical expressions. \n",
    "\n",
    "Compare the theoretical asymptotics of the memory usage for the backpropagation of the modified Gram-Schmidt algorithm with every approach listed above. \n",
    "What data is needed for backpropagation and thus affects the memory consumption?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivjX9_-i3BD0"
   },
   "source": [
    "### Your solution\n",
    "$\\frac{\\partial y}{\\partial x} = $\n",
    "\n",
    "$\\frac{\\partial u_k}{\\partial v_j} = $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7dlEMDu3EDc"
   },
   "outputs": [],
   "source": [
    "def backward_normalization(...):\n",
    "    # Your code \n",
    "\n",
    "def backward_orthogonalization(...):\n",
    "    # Your code \n",
    "\n",
    "class QR():\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Computes QR decomposition of matrix A\n",
    "\n",
    "        Input: \n",
    "            A - n x m matrix\n",
    "        Output:\n",
    "            Q - n x m orthonormal matrix\n",
    "            R - m x m upper triangular matrix\n",
    "        \"\"\"\n",
    "\n",
    "        # Your code\n",
    "    \n",
    "        return Q, R\n",
    "        \n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Computes QR decomposition of matrix A\n",
    "\n",
    "        Input: \n",
    "            grad_output - n x m matrix, derivative of the previous layer (derivative of loss dL/dQ  in our case)\n",
    "        Output:\n",
    "            grad_input - n x m derivative dL/dA\n",
    "        \"\"\"\n",
    "        # Your code\n",
    "         \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWTu_qrh3L5z"
   },
   "outputs": [],
   "source": [
    "qr = QR()\n",
    "A = torch.rand((m, n))\n",
    "\n",
    "Q, R = qr.forward(A)\n",
    "loss1 = Q.sum()\n",
    "\n",
    "# TODO compute losses and do backpropagation\n",
    "# compute dL / dQ and finally dL / dA\n",
    "...\n",
    "dL_dA = ...\n",
    "\n",
    "\n",
    "A_autograd = A.clone().detach().requires_grad_(True)\n",
    "Q_autograd, R_autograd = qr.forward(A_autograd)\n",
    "loss2 = Q_autograd.sum()\n",
    "loss2.backward()\n",
    "print('Difference between gradients:', float(torch.linalg.norm(dL_dA - A_autograd.grad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSj7bUj03S2W"
   },
   "source": [
    "### Bonus tasks\n",
    "\n",
    "__6.__ (10 points) Modify the previous code to be memory-efficient. \n",
    "\n",
    "Hint: not all intermediate variables need to be stored for backpropagation.\n",
    "\n",
    "__7.__ (3 points) Again, measure the required memory for backpropagation. Explain the theoretical asymptotics of the memory usage for the memory-efficient backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lybYxQ214EoQ"
   },
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xb5PD8726joj"
   },
   "source": [
    "## Problem 2. Voice classification (40 pts)\n",
    "\n",
    "In this assigment you will have a chance to develop a voice-anti-spoofing algorithm, enhancing your skills in standard PyTorch classificaton development pipeline.\n",
    "\n",
    "There are several types of attack: \n",
    "- replay attacks, when one records and repeats someone's voice, \n",
    "- impresario attacks, when the speaker mimics the target's voice like an actor and \n",
    "- the most interesting are deepfake attack, based on text-to-speech, voice cloning and voice conversion algorithms. \n",
    "\n",
    "In this work we will focus on deepfake attack and try to differ them from genuine (or bona-fide) speech.\n",
    "\n",
    "We will use data, provided by organisers of biannual [ASVspoof](https://www.asvspoof.org/) contest. More precisely, we will take subpart `ASVspoof_2019LA` dataset, which consist of 17 different attacks DF attack types and bona-fide. All recordings were augmented to be close to real telephone recordings. We will not take 2021 data, but if you want, you can use 2021 data as test and 2019 data as train and validation.\n",
    "\n",
    "The initital dataset can be downloaded from [here](https://datashare.ed.ac.uk/handle/10283/3336). However, we [provide you smaller subsample](https://drive.google.com/drive/folders/1-CyCFA3komqrtyoYj21y5gzfh_vcKBYx?usp=share_link) of dev (val) and eval (test) parts to make work in colab relatively possible :) . You can download it locally or put into you Drive. You will need around 3GB free space, think about creating a new account if you don't have enough.\n",
    "\n",
    "We encourage you to look through data [overview](https://datashare.ed.ac.uk/bitstream/handle/10283/3336/asvspoof2019_evaluation_plan.pdf?sequence=1&isAllowed=y) and result summary [paper](https://datashare.ed.ac.uk/bitstream/handle/10283/3336/asvspoof2019_Interspeech2019_submission.pdf?sequence=2&isAllowed=y).\n",
    "\n",
    "One important thing to notice: main application of this algorithms is in voice-biometry, when we want to stop illegal intruder. Thus, there are two (or 3) common ways of developing VAS algorithms: speaker-aware, we we train verification model which is sensitive to spoofing and differs bona-fide and impostor or we combinde verification score and score of VAS model to decide, whetherr the person is the same, and finally simple and general countermeasures setup, when given audio and model should predict whether it is a spoof or bona-fide. We will work on the last setup.\n",
    "\n",
    "**Important: feel free not to use pre-defined functions, you can solve the task as you wish or change functions/pipelines significantly.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhgZfMu8VObW"
   },
   "source": [
    "#### Preparations\n",
    "It is a good practice to log and check you experiments. In this assignemnt, we ask you to sign up in wandb, log your experiments and give us a link to your project and attach plots/report in the notebook with your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cr0giQiPVS-3"
   },
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import IPython.display as ipd\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "# import librosa\n",
    "import sklearn.metrics as metrics\n",
    "import wandb\n",
    "from torchvision.models import resnet50, resnet18\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import sigmoid\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hngpu66vVbZv"
   },
   "outputs": [],
   "source": [
    "# https://wandb.ai/quickstart\n",
    "\n",
    "!wandb login  # and paste your API key from https://wandb.ai/authorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uswFfpAtVdji"
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"voice-anti-spoofing\", name = \"test_run\", tags = [\"SmallModel\", \"LA\"])\n",
    "SEED = 42  # do not change it\n",
    "# mount drive if you wish\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "%cd /content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwlxTkBoVi83"
   },
   "outputs": [],
   "source": [
    "!ls for_colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T92oyJj_Vl5o"
   },
   "outputs": [],
   "source": [
    "# to make dataframes\n",
    "path_la_train = \"for_colab/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\n",
    "path_la_dev = \"for_colab/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\"\n",
    "path_la_eval = \"for_colab/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt\"\n",
    "\n",
    "flac_la_train = \"for_colab/train_flac/\"\n",
    "flac_la_dev = \"for_colab/flac_dev/\"\n",
    "flac_la_eval = \"for_colab/flac_eval/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUUYCfLPVnzy"
   },
   "outputs": [],
   "source": [
    "columns = [\"speaker_id\", \"audio_file_name\", \"system_id\", \"skip\", \"class\"]\n",
    "df = pd.read_csv(path_la_train, sep=\" \", header=None)\n",
    "df.columns = columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9gD9ZaFVqcx"
   },
   "outputs": [],
   "source": [
    "def makedf_ultimate(txt_file_path, flac_path, dataset_name='ASVspoof', data_type='LA', year=2019):\n",
    "    \n",
    "    if dataset_name=='ASVspoof':\n",
    "        name = dataset_name+str(year)+data_type\n",
    "        if data_type=='DF':\n",
    "            attack_name = 'deepfake'\n",
    "        elif data_type=='LA':\n",
    "            attack_name = 'deepfake'\n",
    "        else:\n",
    "            attack_name = 'replay'\n",
    "            \n",
    "        df = pd.read_csv(txt_file_path, sep=\" \", header=None)\n",
    "\n",
    "            \n",
    "        if year == 2019:\n",
    "            df.columns = [\"speaker_id\", \"audio_file_name\", \"system_id\", \"skip\", \"class\"]\n",
    "            df['dataset'] = name\n",
    "            di = {'bonafide':0, 'spoof':1} # assign classes\n",
    "\n",
    "            df['class'] = df['class'].map(di)\n",
    "            # m = df['class'] == 0\n",
    "            # df.loc[m, 'attack'] = 'noattack'\n",
    "            df['audio_path'] = flac_path + df['audio_file_name'] + '.flac'\n",
    "            df = df[[\"speaker_id\", \"audio_path\", \"class\"]]\n",
    "        \n",
    "    # df = df[['audio_path', 'class', 'dataset', 'attack']]\n",
    "    df = df[[\"speaker_id\", \"audio_path\", \"class\"]]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75LehptwVrGd"
   },
   "outputs": [],
   "source": [
    "df_train = makedf_ultimate(path_la_train, flac_la_train, year=2019, data_type='LA')\n",
    "df_dev = makedf_ultimate(path_la_dev, flac_la_dev, year=2019, data_type='LA')\n",
    "df_eval = makedf_ultimate(path_la_eval, flac_la_eval, year=2019, data_type='LA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waWjZX6pWAsv"
   },
   "source": [
    "**Task 1.1 (2 points)** Check data distribution. Is it imbalanced? How are you going to deal with it?\n",
    "\n",
    "Hints: upsampling, downsampling, add new data, e.g. from this [source](https://commonvoice.mozilla.org/) (but will new audios have the same distribution?), or adjust loss functions.\n",
    "\n",
    "**Answer**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41RI-j9uVs7K"
   },
   "outputs": [],
   "source": [
    "# for dev and eval, let's simplify the task and use smaller amount of data, but balanced to better understand metrics\n",
    "# we need this functions, because it removes unuploaded data from our dataframes\n",
    "def balanced_split(df):\n",
    "    k = min(len(df[df['class']==0]), len(df[df['class']==1]))\n",
    "    print(k)\n",
    "    df_bf = df[df['class']==0].sample(frac=1, random_state=SEED).iloc[0:k]\n",
    "    df_spoof = df[df['class']==1].sample(frac=1, random_state=SEED).iloc[0:k]\n",
    "\n",
    "    df_bf = df_bf.append([df_spoof], ignore_index=True)\n",
    "    df_bf = df_bf.reset_index(drop=True)\n",
    "    return df_bf\n",
    "\n",
    "df_eval = balanced_split(df_eval)\n",
    "df_dev = balanced_split(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMWO8frNWJwO"
   },
   "outputs": [],
   "source": [
    "# for audio manipulations we advise you to use torchaudio or librosa\n",
    "x, sr = torchaudio.load(df_eval[\"audio_path\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dHNyyYeWL33"
   },
   "outputs": [],
   "source": [
    "ipd.display(ipd.Audio(x, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilgomkb8WP7u"
   },
   "source": [
    "**Task 1.2. (1 point)** Display several spoof and bona-fide audios. Can you hear the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J--_dFb5WNeM"
   },
   "outputs": [],
   "source": [
    "### Your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CjF_z7hWV6r"
   },
   "source": [
    "### Custom dataset and Model\n",
    "\n",
    "You can find inspiration and hints in \n",
    "- https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html\n",
    "- https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "- https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "- https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "- https://pypi.org/project/audiomentations/ \n",
    "- https://pytorch.org/audio/stable/tutorials/audio_data_augmentation_tutorial.html\n",
    "- and any other blogposts about spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR8u_nfvWfO6"
   },
   "source": [
    "**Task 2.1 (5 points)** We can work with audios as with images, transforming into spectrograms. Here your task is to implement simple model, which receives raw wav (amplitudes, but probably already preprocced in dataset), transforms it into mel-spectrogram, changes amplitude to DB scale (if you want) and then procceses through the layers. You are free to use ready pre-trained backbones, e.g. from `torchvision.models` and fine-tune them. However, if you want to practise more, create something on your own. Large models with `requires_grad=True` for all parameters will probably result in problems on training in colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_a_qzmwPWkm-"
   },
   "outputs": [],
   "source": [
    "class WavResNet(nn.Module):\n",
    "    def __init__(self, classes=None, resample=16000, n_mels=80, melspec_config=None):\n",
    "        super().__init__()\n",
    "        self.fbank = ...\n",
    "        self.to_db = ...\n",
    "        \n",
    "        model = ...\n",
    "        model.conv1 = ...\n",
    "        num_ftrs = ...\n",
    "        model.fc = nn.Linear(num_ftrs, classes)\n",
    "        self.network = model\n",
    "        # or create your own layers and use them in forward pass\n",
    "        \n",
    "    \n",
    "    def forward(self, x, wav_lens: Optional[torch.Tensor]=None):  # can check the length if you want. this is helpful for inference\n",
    "        mels = ...\n",
    "        mels_db = ...\n",
    "        mels_db = ...\n",
    "        out = ...\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-r5R8d6cWny4"
   },
   "source": [
    "**Task 2.2 (1 point)**\n",
    "In your opinion, which approach is better for binary classification: \n",
    "- Model's last layer output has shape 1, train with BCE-like loss.\n",
    "- Model's last layer output has shape 2, train with cross-entropy like loss. \n",
    "\n",
    "**Task 2.3 (3 points)** Create custom dataset, which recieves ```df``` and returns preprocessed audio.\n",
    "\n",
    "**Task 2.4 (1 point)** Should we use augmentaions? If yes, which ones? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMY3RGZ_WzbI"
   },
   "outputs": [],
   "source": [
    "class DatasetWav(Dataset):\n",
    "\n",
    "    def __init__(self, data_frame, padding_sec=4, default_sr=16000, transform=None):\n",
    "        self.df = data_frame\n",
    "        self.padding_sec = padding_sec\n",
    "        self.default_sr = default_sr\n",
    "        self.labels = ...\n",
    "        self.paths = ...\n",
    "        self.vad = ...   # in this task you are free to ommit it in order to speed up calculations,\n",
    "                         # also the provided data should be rather clean\n",
    "    def __len__(self):\n",
    "        return \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        label = ...\n",
    "        path = ...\n",
    "        waveform , sr = ... # normalize=True\n",
    "        # transform waveform from stereo to mono channel \n",
    "        waveform = ...\n",
    "        resample_transform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.default_sr) # should we use it for our data or can ommit? \n",
    "        waveform = ...\n",
    "        # waveform = self.vad(waveform)\n",
    "\n",
    "        # came up with idea, what to do if audio is longer or shorter than reuqired\n",
    "        if (len(waveform) < self.padding_sec * self.default_sr):\n",
    "          pass\n",
    "        else:\n",
    "          pass\n",
    "            \n",
    "        return x, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSj8MUJkW0wy"
   },
   "outputs": [],
   "source": [
    "# check that works\n",
    "batch_size = None\n",
    "train_dataset_wavs = DatasetWav(df_train) #.iloc[0:200]\n",
    "train_dataloader_wavs = DataLoader(train_dataset_wavs, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "x, y = next(iter(train_dataloader_wavs))\n",
    "model = ...\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhzKww_IW5Ej"
   },
   "source": [
    "### Train and Test functions\n",
    "**Task 3.0 (1 point)** What is the difference between `model.train()` and `model.eval()`? Does `model.eval()` mode take gradient statisitcs into account?\n",
    "\n",
    "**Task 3.1 (5 points)** Implement train and test functions, which iterate over all batches. Do logging of loss, accuracy on each batch and after every epoch. Check equal error rate EER, explain, what is it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtKSHUeEW3EM"
   },
   "outputs": [],
   "source": [
    "def calculate_eer(y, y_score):\n",
    "  fpr, tpr, thresholds = roc_curve(y, y_score, pos_label=1)\n",
    "  eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "  thresh = interp1d(fpr, thresholds)(eer)\n",
    "  return eer, thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOArRccuW-i3"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloaders, criterion, optimizer, \n",
    "          num_epochs=3, scheduler=None, savename=None,\n",
    "          print_counter=10, decay_factor=10,\n",
    "          device=torch.device(\"cuda\")):\n",
    "    pass\n",
    "\n",
    "def test(model, test_dataloader, criterion=None, device=None, savename=None):\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBWhPKj1XDJo"
   },
   "source": [
    "### Main loop \n",
    "\n",
    "**Task 4.1 (3 points)** What loss will you choose and why? What it should receive for input (logits, probs)? Explain your answer.\n",
    "Consider also:\n",
    "- https://medium.com/swlh/focal-loss-what-why-and-how-df6735f26616\n",
    "- https://libauc.org/\n",
    "\n",
    "\n",
    "**Task 4.2 (5 points)** Implement main function, which receives yaml or json config (or path to it) and train and test the model, save model checkpoints, and model's test predictions. At least, you should obain results better than random. It is good, if accuracy on test dataset is >= 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ceTCHQBXWcB"
   },
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    lr =  \n",
    "    epochs = \n",
    "    batch_size = \n",
    "    # wandb.init(project=\"voice-anti-spoofing\", name = \"test_run\", tags = [\"ResNet\", \"LA\"])\n",
    "    wandb.config = {\n",
    "      \"learning_rate\": lr,\n",
    "      \"epochs\": epochs,\n",
    "      \"batch_size\": batch_size\n",
    "    }\n",
    "\n",
    "    \n",
    "    # debug firstly on small subpart of dataset\n",
    "    train_dataset_wavs = DatasetWav() #.iloc[0:200]\n",
    "    train_dataloader_wavs = DataLoader()\n",
    "    val_dataset_wavs = \n",
    "    val_dataloader_wavs = \n",
    "    test_dataset_wavs =\n",
    "    test_dataloader_wavs = \n",
    "    dataloaders = {\"train\": train_dataloader_wavs, \"val\": val_dataloader_wavs}\n",
    "    \n",
    "    \n",
    "    device = \n",
    "    classes =                                             \n",
    "    model =                                                                              \n",
    "    model = model.to(device)\n",
    "    # freeze layers except the last one if you want\n",
    "    for param in model.parameters():\n",
    "      param.requires_grad = ...\n",
    "    model.network.fc.requires_grad_(...)\n",
    "                                                                                           \n",
    "    optimizer = \n",
    "    scheduler = \n",
    "    sc = torch.tensor([alpha, 1 - alpha]).to(device) # weights for loss\n",
    "    criterion = nn.CrossEntropyLoss(weight=sc)\n",
    "    \n",
    "    for_ckpts = \".\"\n",
    "    os.makedirs(for_ckpts, exist_ok=True)\n",
    "    \n",
    "    model, val_acc_history = train(...)\n",
    "    \n",
    "    y, probs = test(...)\n",
    "    \n",
    "    return y, probs\n",
    "\n",
    "y, probs = main(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrnoNpU8XdsU"
   },
   "source": [
    "Training might require a long time. So, you can try to use subpart of the dataset to receive rather good results. Also, you don't have to train for many epochs, 1 or even a half of epoch might be enough. Also consider to make some speedups in the model and dataset.\n",
    "\n",
    "Colab also might work unstable with lots of data. If this happens, think how to deal with it, e.g. restart the kernel and/or remove the majority of data from Drive and try to bugfix firstly, then do train and test separately. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3BT6qC2YQvG"
   },
   "source": [
    "**Task 4.3 (5 points)** Analyze the results. Play some audios with wrong predictions. Plot prediction distributions vs class. Is it possible to find better threshold?\n",
    "\n",
    "**Task 4.4 (5 points)** Discuss the results and your approaches + hyperparameters. If you can, run some experiments with different parameters.\n",
    "\n",
    " Write your ideas, what else you can try to improve the results in the future.\n",
    "\n",
    "**Task 4.5 (3 points)** Provide link to wandb project, or link to report. Upload best model's weights to Drive and provide a link with access right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbxOjJDiY1Vi"
   },
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz1krNfvYsBa"
   },
   "source": [
    "**Bonus task. (50 points)** \n",
    "\n",
    "So far, we didn't care about speaker_id and verification. Take verification model from whatever you want and create counter measure subnetwork, as described in this [article](https://sasv-challenge.github.io/pdfs/2022_descriptions/IDVoice.pdf). Train the model, as described there. ASVspoof also provides Enrollment(anchor) recordings. Try to make blending, to obtain better metrics for counter measures.\n",
    "\n",
    "These links might be helpful:\n",
    "- https://github.com/archinetai/surgeon-pytorch\n",
    "- https://www.kaggle.com/code/peter0749/additive-margin-softmax-loss-with-visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gPwK-nRqmkTt"
   },
   "source": [
    "## Problem 3. (30 pts)\n",
    "\n",
    "### Task 1 (10 pts)\n",
    "\n",
    "For FCNN (Fully connected Neural Networks)\n",
    "\\begin{equation}\n",
    "f(x)=f_k\\left(f_{k-1}\\left(\\ldots\\left(f_0(x)\\right)\\right),\\right.\n",
    "\\end{equation}\n",
    "assume the nonlinearity function $\\sigma$ is ReLU and loss function $L$ is squared error loss function.\n",
    "Prove that the deep learning model is not unique for any datasets, i.e. the optimization problem \n",
    "\\begin{equation}\n",
    "\\min_\\theta \\ g(\\theta)=\\frac{1}{N} \\sum_{i=1}^N L\\left(y_i, \\hat{y}_i\\right), \\text{where} \\ \\ \\hat{y}_i=f\\left(x_i, \\theta\\right),\n",
    "\\end{equation} \n",
    "dose not have an unique global minimizer $\\theta$.\n",
    "\n",
    "### Task 2 (5 pts)\n",
    "\n",
    "The *softmax* activation function is \n",
    "\\begin{equation}\n",
    "\\mathrm{Softmax}(\\mathbf{z})_i=\\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}} \\ \\text { for } \\  i=1, \\ldots, K \\text { and } \\mathbf{z}=\\left(z_1, \\ldots, z_K\\right) \\in \\mathbb{R}^K.\n",
    "\\end{equation}\n",
    "Suppose $\\mathbf{z} \\in \\mathbb{R}^K$, and $\\mathbf{a} \\in \\mathbb{R}^K$,\n",
    "Prove that for the optimization problem\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\min_{\\mathbf{a}} \\ -\\langle\\mathbf{a}, \\mathbf{z}\\rangle+\\langle\\mathbf{a}, \\log \\mathbf{a}\\rangle \\\\\n",
    "& \\text { s.t. } \\sum_k^K \\mathbf{a}_{k}=1,\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "the minimizer is \n",
    "\\begin{equation}\n",
    "\\mathbf{a}^*=\\mathrm{Softmax}(\\mathbf{z}).\n",
    "\\end{equation}\n",
    "\n",
    "*Hint*: KKT optimality conditions and convexiy of the problem can help you to show the required equality\n",
    "\n",
    "### Task 3 (15 pts)\n",
    "\n",
    "For function $f(x)=x^2, x \\in \\left[0, 1\\right]$, \n",
    "\n",
    "1) prove that the neural network $\\hat f(x)$ based on the following structure has the approximation error \n",
    "\\begin{equation}\n",
    "|f(x)-\\hat f(x)|\\leq \\frac{1}{2^{n-1}}\n",
    "\\end{equation}\n",
    "where $n$ is the number of layers in the neural network.\n",
    "\n",
    "![](neural_parabola.png)\n",
    "\n",
    "*Hint*: For each $ x \\in \\left[0, 1\\right]$, $x$ can be denoted by its binary expansion $x=\\sum_{i=0}^{∞} x_i/2^i$, where $x_i \\in \\{ 0, 1\\}$. The above structure can be used to find $x_0,\\dots, x_n$. Then we can write $\\hat f(x)=f\\left(\\sum_{i=0}^{∞} x_i/2^i\\right).$\n",
    "\n",
    "After the proof, if we want to achieve $\\epsilon$ appoximation error based on the above neural network, the number of layers $n$ has to satisfy the condition $\\frac{1}{2^{n-1}}\\leq \\epsilon$, i.e. $n\\geq \\log_2 \\frac{1}{\\epsilon}$.\n",
    "\n",
    "2) Implement this neural network in any framework you like with different $n$ (for example $n = 3, 5, 10, 15$), and then plot the curve for absolute errors for different $n$. Compare the obtained plots with theoretical bound.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-ljPkkgA8QI"
   },
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
