{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 4: СV tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Previous lecture:  Training better\n",
    "- SGD optimization methods (SGD with momentum, Adam, ...)\n",
    "- Problems with training deep models: vanishing gradients, catastrophic forgetting\n",
    "- Effect of initializations\n",
    "- Normalizations \n",
    "- Early stopping \n",
    "- Interesting properties of loss surfaces of DNN models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Todays lecture\n",
    "\n",
    "- object detection\n",
    "- semantic segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The most popular CV tasks\n",
    "\n",
    "The most popular computer vision (CV) task is classification.\n",
    "\n",
    "We already discussed the supervised learning setting, loss functions (we put softmax to predict probabilities),\n",
    "\n",
    "and we train them on large datasets using SGD-type methods \n",
    "\n",
    "(with actually a lot of tricks which we will discuss later, both to improve generalization **and** computational efficiency)\n",
    "\n",
    "Basic architectures were also discussed before (**CNN**, **ResNet**).\n",
    "\n",
    "**Visual Transformer** models will be discussed later in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Brief reminder: evaluation metrics for classification tasks, accuracy, precision, recall\n",
    "<img src='metrics.webp' >\n",
    "\n",
    "Typically, we talk about accuracy (percentage of correct predictions). Works well when classes are **balanced** (i.e. have similar number of samples).\n",
    "\n",
    "**Accuracy:**\n",
    "$$A = \\frac{TP+TN}{TP+FP+TN+FN}.$$\n",
    "**Precision**\n",
    "$$P = \\frac{TP}{TP+FP}.$$\n",
    "**Recall**\n",
    "$$R = \\frac{TP}{TP+FN}.$$\n",
    "**F1 score (harmonic mean between precision and recall)**\n",
    "$$F1 = 2 \\frac{R \\circ P}{P + R}.$$\n",
    "\n",
    "Finally, **ROC (Receiver operating curve)** is a plot between \n",
    "True Positive rate (**TPR** = Recall) and False Positive Rate (**FPR**), \n",
    "\n",
    "$FPR = \\frac{FP}{TN+FP}.$\n",
    "\n",
    "\n",
    "We change the threshold between positive class and negative class.\n",
    "AUC is the area under the curve. We want to make it bigger.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Object detection.\n",
    "\n",
    "The goal of object detection is (!) detect objects on images. The detection includes: \n",
    "- the detection of the bounding box of the object.\n",
    "- the detection of the class of the object.\n",
    "\n",
    "<img src='object-detection.jpeg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of object detection\n",
    "\n",
    "- Number 1: FaceID systems use face detection module in the first place\n",
    "- Autonomous driving \n",
    "- Medical applications\n",
    "- Robotics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How the data looks like.\n",
    "\n",
    "There are quite a lot of datasets for object detection, [see paperswithcode](https://paperswithcode.com/task/object-detection)\n",
    "\n",
    "- Subset of imageNet has object locations (i.e. multiple objects in the same photo, approximately 3 per image, in total 1,034,908 images, [see paperswithcode](https://paperswithcode.com/dataset/imagenet)).\n",
    "- MS Coco dataset\n",
    "- Smaller (kind of 'MNIST' is Pascal VOC.\n",
    "- Not too many new ones (probably superseeded by semantic segmentation as more complex task, I am not actually sure why)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How we evaluate object detection models\n",
    "\n",
    "The prediction of the boxes is measured using IoU measure.\n",
    "\n",
    "The prediction of the classes is computed using mean average precision (mAP) measure:\n",
    "\n",
    "The reason is that we can have different classes in one image, and different thresholds have to be used.\n",
    "\n",
    "\n",
    "<img src='IoU.jpeg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Average precision\n",
    "\n",
    "In order to compute average precision (for object detection), we compute **precision-recall** curve for different threshold, i.e. if $IoU > t$ the class is positive, if this is not satisfied -- the class is negative. \n",
    "\n",
    "Once precision and recall values are computed, we compute\n",
    "\n",
    "$$AP = \\sum_{k=0}^{k-n} [R(k)-R(k+1)] P(k),$$\n",
    "which is just the **area under the curve** for precision-recall. \n",
    "AP is always between 0 and 1 (check!)\n",
    "\n",
    "\n",
    "In object detection we have different classes, and they can use different thresholds!\n",
    "\n",
    "We just average AP for different classes and thats all!\n",
    "\n",
    "This is a standard metric for benchmarking object detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Architectures\n",
    "\n",
    "- Bunch of successful classical detectors (Viola-Jones, SIFT/Hog, used for face detection mainly)\n",
    "\n",
    "- First succesfull CNN architecture has been [R-CNN architecture](https://arxiv.org/abs/1311.2524).\n",
    "\n",
    "- R-CNN stands for region proposal CNN\n",
    "\n",
    "- YOLO (You look only once) model family \n",
    "\n",
    "- Modern: transformer-based models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## R-CNN\n",
    "\n",
    "The original R-CNN has the flavour of classical methods. \n",
    "\n",
    "First, the authors used **selective search** (which looks at things like histograms to measure the similarity between regions) to select large number (2000) of candidates. \n",
    "\n",
    "Then, these regions are warped into a square and fed into a CNN which generates a vector of dimension **4096**.\n",
    "\n",
    "The extracted features are fed into SVM to classify if the object is in the region or not.\n",
    "\n",
    "The algorithm also predicts the offset values (4 of them) for the object bounding box.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problems with R-CNN\n",
    "\n",
    "- Fixed selective search algorithm\n",
    "- Very long training\n",
    "- Inference is not real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next step: Fast R-CNN\n",
    "\n",
    "In the fast R-CNN model:\n",
    "\n",
    "- Do the CNN first\n",
    "- Do selection search on features (still you see it here, authors did not believe you can get rid of it or they really liked it!)\n",
    "- Since convolution has limited reception field, the original objects are mapped to some (bounded) objects in the feature map\n",
    "- Faster than R-CNN and slightly more accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next step(2): Faster R-CNN\n",
    "\n",
    "- Key idea was to replace the selective search algorithm by another neural network that predicts the region.\n",
    "- The layer is **region proposal network**\n",
    "- The loss is classification (for object) and regression (for boundaries).\n",
    "- ROI pooling layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"display: flex\">\n",
    "    <div style=\"flex: 1; margin-right:10px; margin-top: 100px; width: 100%\">  \n",
    "        <img src=\"rpn.png\" width=\"80%\">\n",
    "    </div>\n",
    "    <div style=\"flex: 1; margin-top: 0px\">\n",
    "        <img src=\"faster-rcnn.jpeg\" width=\"100%\">\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of R-CNN family\n",
    "\n",
    "The Faster R-CNN has become much faster (more than 100x) and could be used for real-time detection.\n",
    "\n",
    "You can change backbones as well to more modern architectures easily!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## YOLO models\n",
    "- In R-CNN models, neural networks first look for regions where the object can be located and works with the regions.\n",
    "\n",
    "- In YOLO (You look only once) a single CNN predicts the bounding boxes and classes simultaneously\n",
    "- The input is split into $S \\times S$ grid.\n",
    "- For each cell we predict $B$ bounding boxes. Each box is $(x, y, w, h, confidence)$.  The grid prediction is done through the size of the output tensor, which is $S \\times S \\times (5 B + C)$.\n",
    "\n",
    "<img src='YOLO.webp' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Differences between YOLO and R-CNN\n",
    "- YOLO is much faster\n",
    "- YOLO looks globally at the image in a single CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## YOLOv2\n",
    "\n",
    "In YOLOv2 several improvements have been made:\n",
    "\n",
    "- Different backbone based on VGG\n",
    "- Anchor boxes (predefined boxes of different size + predicted BB).\n",
    "- Batch normalization\n",
    "\n",
    "Faster and more accurate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next generations of YOLO\n",
    "\n",
    "- 2016: YOLOv3 ResNet architecture (called **Darknet-53**) and Feature Pyramid networks. \n",
    "- 2018: YOLOv4 (developed not by original team): new architecture (called CSPNet, based on **DenseNet**) + new loss  (to improve on imbalanced datasets).\n",
    "- 2020: YOLOv5 (developed by original team) -- EfficientDet (based on EfficientNet).\n",
    "- 2022: YOLOv6, new backbone + new method for anchor prediction.\n",
    "- 2023: YOLOv7: **focal loss** $FL(p_t) = (1-p_t)^{\\gamma} \\log p_t$, putting more weight on missclassified examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## VIT\n",
    "As a backbone, you can also use vision transformers as a backbone(will discuss if time permits and I will not forget to do it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of this part\n",
    "- There are different families for efficient object detection (R-CNN & YOLO). \n",
    "- They do have some problems, but overall the quality is growing due to extensive engineering efforts\n",
    "- The dataset collection is also important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Image segmentation\n",
    "\n",
    "There are several types of image segmentation. \n",
    "\n",
    "That includes:\n",
    "\n",
    "- **Semantic segmentation** (every pixel belongs to a specific data)\n",
    "- **Instance segmentation** (separation of the image into regions with classes)\n",
    "- **Panoptic segmentation** (predicts the identity of each object)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Semantic segmentation\n",
    "\n",
    "In semantic segmentation, we need to predict the class of each pixel (i.e. all trees, all persons, etc.). \n",
    "This is an example of **image-to-image** transformation: as an input, we have an image, as the output we have the mask\n",
    "\n",
    "<img width=60%, src='semantic-seg.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instance segmentation\n",
    "\n",
    "In **instance segmentation**, we need to predict not only the class, but also the instances of the object. \n",
    "\n",
    "Similar to object detection, but \n",
    "\n",
    "\n",
    "<img width=80%, src='instance.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Panoptic segmentation\n",
    "\n",
    "Panoptic segmentation combines best of both worlds.\n",
    "\n",
    "\n",
    "Each pixel in a scene is assigned a semantic label\n",
    "\n",
    "(due to semantic segmentation) and a unique instance identifier (due to instance segmentation).\n",
    "\n",
    "<img width=80%, src='panoptic.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation metrics for image segmentation\n",
    "\n",
    "- IoU (already covered) for semantic segmentation\n",
    "- Instance segmentation is measured using **average precision** \n",
    "- Panoptic uses **Panoptic Quality** which evaluates the predicted masks and instance identifiers. PQ unifies evaluation over all classes by multiplying segmentation quality (SQ) and recognition quality (RQ) terms. SQ represents the average IoU score of the matched segments, while RQ is the F1 score calculated using the precision and recall values of the predicted masks.\n",
    "- DICE coefficient which is simply **two times the overlap area divided by the total number of pixels in both images**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Architectures for semantic segmentation\n",
    "There are quite a few architectures for semantic segmentation including:\n",
    "\n",
    "- SegNet \n",
    "- U-Net \n",
    "- FCNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SegNet\n",
    "SegNet was the first architecture and had the following form of the **image-to-image** (pix2pix) architecture\n",
    "\n",
    "<img src='segnet.webp' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## U-Net\n",
    "\n",
    "U-Net is still one of the most popular architectures for image-to-image models and is used in diffusion models.\n",
    "\n",
    "The key idea is to add skip connections to ensure **multiscale** processing of the input data.\n",
    "\n",
    "<img src='unet.webp'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dilated convolutions\n",
    "\n",
    "Several backbones (DeepLab v.1,2, 3) use dilated (atrous) convolutions to build the encoder, for example https://arxiv.org/pdf/1706.05587v3.pdf\n",
    "\n",
    "These convolutions replace pooling, which is good for **abstract features** but not for **pixel predictions**\n",
    "\n",
    "<img src='deeplab.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## FastFCN\n",
    "\n",
    "A [Fast Fully Convolutional network](https://arxiv.org/abs/1903.11816) based on DeepLabv3 is used to replace dilated convolutions (which take a lot memory and time). \n",
    "The original FCN paper uses upsampling with **dilated convolutions** \n",
    "The fast version takes the last 3 conv layers and uses **Joint Pyramid Upsampling** from those layers. \n",
    "\n",
    "<img src='FastFCN.webp'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Losses for image segmentation\n",
    "\n",
    "There are quite a lot of different losses for the segmentation problems, see [review](https://arxiv.org/pdf/2006.14822.pdf)\n",
    "\n",
    "| Type | Loss Function |\n",
    "| --- | --- |\n",
    "| Distribution-based Loss | Binary Cross-Entropy |\n",
    "|  | Weighted Cross-Entropy |\n",
    "|  | Balanced Cross-Entropy |\n",
    "|  | Focal Loss |\n",
    "|  | Distance map derived loss penalty term |\n",
    "| Region-based Loss | Dice Loss |\n",
    "|  | Sensitivity-Specificity Loss |\n",
    "|  | Tversky Loss |\n",
    "|  | Focal Tversky Loss |\n",
    "|  | Log-Cosh Dice Loss |\n",
    "| Boundary-based Loss | Hausdorff Distance loss |\n",
    "|  | Shape aware loss |\n",
    "| Compounded Loss | Combo Loss |\n",
    "|  | Exponential Logarithmic Loss. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Losses for segmentation\n",
    "\n",
    "Since segmentation can be viewed as classification, the distribution-based losses (including the focal loss) are clear.\n",
    "\n",
    "One can also use the differentiable version of the Dice loss, defined as\n",
    "\n",
    "$$DL(y, \\hat{p}) = 1 - \\frac{2 y \\hat{y} + 1}{y + \\hat{y}+1}$$ \n",
    "\n",
    "It is claimed that it works better for imbalanced datasets (typically, medical data).\n",
    "\n",
    "Many other losses and a lot of engineering! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Architectures for instance segmentation\n",
    "Not lets discuss **Instance Segmentation**.\n",
    "Some of the popular architectures include:\n",
    "\n",
    "- Mask R-CNN\n",
    "- PANet\n",
    "- YOLACT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mask R-CNN\n",
    "\n",
    "Mask-R-CNN (Detectron by Facebook, now [Detectron2](https://github.com/facebookresearch/detectron2).\n",
    "\n",
    "The difference with Detectron and Detectron2 is that the latter is written in Pytorch, includes panoptic segmentation and DensePose.\n",
    "\n",
    "The original architecture is extension of Faster R-CNN described before. \n",
    "\n",
    "The model generates the bounding box and segmentation for each instance. \n",
    "\n",
    "\n",
    "Addinal **head** that predicts the mask! \n",
    "\n",
    "The total loss will be a sum of three losses (class, box, mask).\n",
    "\n",
    "**RoI Align** solves the problem of different size of the feature map and the region map by using interpolation (we have to fit a part of the feature map to a known region).\n",
    "<img src='Mask-R-CNN.webp'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [PANET](https://arxiv.org/pdf/1908.06391.pdf)\n",
    "PANET architecture studies a practical case of **few shot** image segmentation. \n",
    "\n",
    "We will talk about zero-shot/few-shot architectures later on in the course.\n",
    "\n",
    "The idea is that we only have few examples per class.\n",
    "\n",
    "Details later on! (Remind me if I forget about it, since it is useful to explain few-shot classification before few-shot segmentation!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## YOLACT (You only look and coefficients)\n",
    "\n",
    "[YOLACT](https://arxiv.org/abs/1904.02689) is yet another architecture which is quite fast.\n",
    "The design is now quite complicated: it involves prediction of 'semantic vectors' and 'masks'\n",
    "\n",
    "<img src='yolact.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Object detection\n",
    "- Image segmentation \n",
    "- I probably missed a lot, so stay tuned (if not) for CV courses!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture: Learning from sequential data\n",
    "\n",
    "- Recurrent neural networks and their variants (RNN, GRU, LSTM).\n",
    "- The concept of attention\n",
    "- Transformer models"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Слайд-шоу",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "nav_menu": {},
  "rise": {
   "scroll": true,
   "theme": "sky",
   "transition": "zoom"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
